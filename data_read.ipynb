{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and merge csv files (Weather)\n",
    "read_weather = True\n",
    "# process and save processed csv files (Weather)\n",
    "process_weather = True\n",
    "station_name = 'Ardenville'\n",
    "\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# GENERATION\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# read and merge csv files (Generation)\n",
    "read_generation = True\n",
    "# read and merge csv files (Generation)\n",
    "save_process_generation = True\n",
    "asset_name = \"AKE1 McBride Lake Windfarm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(file_path: os.PathLike):\n",
    "    df = pd.read_csv(file_path, encoding='unicode_escape')  # Read CSV file with proper encoding\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Remove any \"Unnamed\" columns that might be auto-generated\n",
    "    return df  # Return the cleaned DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv files and merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved successfully!\n",
      "Merged data has 43843 rows.\n"
     ]
    }
   ],
   "source": [
    "# Read CSVs and drop any unnamed index column\n",
    "if read_weather:\n",
    "    # Create a new folder to save the merged CSV file\n",
    "    os.makedirs('./Data/Merged', exist_ok=True)\n",
    "    # Load cleaned CSVs (each file contains data for a specific time period)\n",
    "    file_path = f\"./Data/ACIS/{station_name}\"\n",
    "    all_weather_data = pd.concat([clean_df(f'{file_path}/{file}') for file in os.listdir(f'{file_path}')], ignore_index=True)\n",
    "    # Save the merged dataframe as a new CSV file without index values\n",
    "    all_weather_data.to_csv(f\"./Data/Merged/{station_name}_weather_data.csv\", index=False)  # `index=False` prevents writing index column to CSV\n",
    "    print('Merged CSV file saved successfully!')\n",
    "    print(f\"Merged data has {all_weather_data.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weather_station_data(df: pd.DataFrame, save: bool = False, columns = ['Date', 'Air Temp. Inst. (Â°C)', 'Humidity Inst. (%)', 'Incoming Solar Rad. (W/m2)','Precip. (mm)', 'Wind Speed 10 m Syno. (km/h)', 'Wind Dir. 10 m Syno. (Â°)', 'Wind Speed 10 m Avg. (km/h)', 'Wind Dir. 10 m Avg. (Â°)',]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get data for a specific asset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(df.columns)\n",
    "        df['Date'] = pd.to_datetime(df['Date (Local Standard Time)'], errors='coerce')        \n",
    "        columns_to_drop = [column for column in df.columns if column not in columns]\n",
    "        df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "        print(df.isna().sum())\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "        if save:\n",
    "            df.to_csv(f'./Data/Merged/{station_name}_weather.csv', index=False)\n",
    "            print('Processed CSV file saved successfully!')\n",
    "        print(\"Miissing values filled\")\n",
    "        print(f\"Data for {station_name} has {df.shape[0]} rows.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"No data found for {station_name}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_32852\\3648812536.py:2: DtypeWarning: Columns (7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='unicode_escape')  # Read CSV file with proper encoding\n",
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_32852\\1026872937.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Station Name', 'Date (Local Standard Time)', 'Air Temp. Inst. (Â°C)',\n",
      "       'Air Temp. Inst. Source Flag', 'Air Temp. Inst. Comment',\n",
      "       'Humidity Inst. (%)', 'Humidity Inst. Source Flag',\n",
      "       'Humidity Inst. Comment', 'Relative Humidity Avg. (%)',\n",
      "       'Relative Humidity Avg. Source Flag', 'Relative Humidity Avg. Comment',\n",
      "       'Incoming Solar Rad. (W/m2)', 'Incoming Solar Rad. Source Flag',\n",
      "       'Incoming Solar Rad. Comment', 'Precip. (mm)', 'Precip. Source Flag',\n",
      "       'Precip. Comment', 'Wind Speed 10 m Syno. (km/h)',\n",
      "       'Wind Speed 10 m Syno. Source Flag', 'Wind Speed 10 m Syno. Comment',\n",
      "       'Wind Dir. 10 m Syno. (Â°)', 'Wind Dir. 10 m Syno. Source Flag',\n",
      "       'Wind Dir. 10 m Syno. Comment', 'Wind Speed 10 m Avg. (km/h)',\n",
      "       'Wind Speed 10 m Avg. Source Flag', 'Wind Speed 10 m Avg. Comment',\n",
      "       'Wind Dir. 10 m Avg. (Â°)', 'Wind Dir. 10 m Avg. Source Flag',\n",
      "       'Wind Dir. 10 m Avg. Comment'],\n",
      "      dtype='object')\n",
      "Air Temp. Inst. (Â°C)            0\n",
      "Humidity Inst. (%)               0\n",
      "Incoming Solar Rad. (W/m2)       2\n",
      "Precip. (mm)                     0\n",
      "Wind Speed 10 m Syno. (km/h)     0\n",
      "Wind Dir. 10 m Syno. (Â°)       88\n",
      "Wind Speed 10 m Avg. (km/h)      6\n",
      "Wind Dir. 10 m Avg. (Â°)        87\n",
      "Date                             0\n",
      "dtype: int64\n",
      "Processed CSV file saved successfully!\n",
      "Miissing values filled\n",
      "Data for Ardenville has 43843 rows.\n",
      "       Air Temp. Inst. (Â°C)  Humidity Inst. (%) Incoming Solar Rad. (W/m2)  \\\n",
      "0                        3.1                71.3                        0.0   \n",
      "1                        3.1                71.3                        0.0   \n",
      "2                        2.9                72.7                        0.0   \n",
      "3                        3.1                69.0                        0.0   \n",
      "4                        4.8                62.0                        0.0   \n",
      "...                      ...                 ...                        ...   \n",
      "43838                   -7.7                92.9                        0.0   \n",
      "43839                   -7.7                93.0                        0.0   \n",
      "43840                   -7.9                92.9                        0.0   \n",
      "43841                   -8.1                92.6                        0.0   \n",
      "43842                   -8.5                92.4                        0.0   \n",
      "\n",
      "       Precip. (mm)  Wind Speed 10 m Syno. (km/h)  Wind Dir. 10 m Syno. (Â°)  \\\n",
      "0               0.0                          32.4                      268.8   \n",
      "1               0.0                          41.9                      273.1   \n",
      "2               0.0                          40.0                      274.7   \n",
      "3               0.0                          31.4                      257.9   \n",
      "4               0.0                          40.4                      276.0   \n",
      "...             ...                           ...                        ...   \n",
      "43838           0.0                           6.7                      108.6   \n",
      "43839           0.0                           7.4                      105.0   \n",
      "43840           0.0                           7.9                       75.2   \n",
      "43841           0.0                           9.2                       56.5   \n",
      "43842           0.0                           8.9                       35.4   \n",
      "\n",
      "       Wind Speed 10 m Avg. (km/h)  Wind Dir. 10 m Avg. (Â°)  \\\n",
      "0                             37.9                     269.7   \n",
      "1                             39.4                     265.8   \n",
      "2                             36.1                     282.9   \n",
      "3                             41.6                     262.0   \n",
      "4                             36.2                     272.9   \n",
      "...                            ...                       ...   \n",
      "43838                          6.7                      84.8   \n",
      "43839                          7.0                     103.6   \n",
      "43840                          8.2                      80.3   \n",
      "43841                          8.8                      57.2   \n",
      "43842                          9.3                      45.6   \n",
      "\n",
      "                     Date  \n",
      "0     2020-01-01 00:00:00  \n",
      "1     2020-01-01 01:00:00  \n",
      "2     2020-01-01 02:00:00  \n",
      "3     2020-01-01 03:00:00  \n",
      "4     2020-01-01 04:00:00  \n",
      "...                   ...  \n",
      "43838 2024-12-31 19:00:00  \n",
      "43839 2024-12-31 20:00:00  \n",
      "43840 2024-12-31 21:00:00  \n",
      "43841 2024-12-31 22:00:00  \n",
      "43842 2024-12-31 23:00:00  \n",
      "\n",
      "[43843 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the merged CSV file\n",
    "weather_data = clean_df(f\"./Data/Merged/{station_name}_weather_data.csv\")\n",
    "if process_weather:\n",
    "    station_data = process_weather_station_data(weather_data, save=True)\n",
    "    # Convert Data\n",
    "    print(station_data)\n",
    "    # Create a new folder to save the merged CSV file\n",
    "    os.makedirs('./Data/Merged', exist_ok=True)\n",
    "    # Fill missing values with the average of previous and next values\n",
    "    # print(station_data.isna().sum())\n",
    "    # print(f\"Processed data has {station_data.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv files and merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved successfully!\n",
      "Merged data has 4866764 rows.\n"
     ]
    }
   ],
   "source": [
    "if read_generation:\n",
    "    # Load data using a for loop and a list comprehension\n",
    "    generation_data = pd.concat([clean_df(f'./Data/CSD/{file}') for file in os.listdir('./Data/CSD')], ignore_index=True)\n",
    "    # Create a new folder to save the merged CSV file\n",
    "    os.makedirs('./Data/Merged', exist_ok=True)\n",
    "    # Save the merged dataframe as a new CSV file without index values\n",
    "    generation_data.to_csv('./Data/Merged/Generation.csv', index=False)  # `index=False` prevents writing index column to CSV\n",
    "    print('Merged CSV file saved successfully!')\n",
    "    print(f\"Merged data has {generation_data.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find assets with most data in the generation data\n",
    "def get_assets_with_most_data(df: pd.DataFrame, fuel_type: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Get the assets with the most data points in the generation data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df[df['Fuel Type'] == fuel_type.upper()]\n",
    "        asset_list = df['Asset Name'].value_counts()\n",
    "        most_valuable_asset = asset_list.where(asset_list == asset_list[0]).dropna()\n",
    "        print(f\"Total number of assets: {len(asset_list)}\")\n",
    "        print(f\"{len(most_valuable_asset)} assets have the most data. They have {most_valuable_asset[0]} data points.\")\n",
    "        return most_valuable_asset\n",
    "    except Exception as e:\n",
    "        print(f\"No data found for {fuel_type} assets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for a specific asset.\n",
    "def get_asset_data(df: pd.DataFrame, asset_name: str, save: bool = False, columns = ['Volume', 'Maximum Capability', 'System Capability']) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get data for a specific asset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        asset_data = df[df['Asset Name'] == asset_name]\n",
    "        columns_to_drop = [column for column in asset_data.columns if column not in columns]\n",
    "        asset_data['Date'] = pd.to_datetime(asset_data['Date (MST)'], errors='coerce')\n",
    "        asset_data.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "        print(asset_data.isna().sum())\n",
    "        asset_data.fillna(method='ffill', inplace=True)\n",
    "        if save:\n",
    "            asset_data.to_csv(f'./Data/Merged/{asset_name}_generation.csv', index=False)\n",
    "            print('Processed CSV file saved successfully!')\n",
    "        print(\"Miissing values filled\")\n",
    "        print(f\"Data for {asset_name} has {asset_data.shape[0]} rows.\")\n",
    "        return asset_data\n",
    "    except Exception as e:\n",
    "        print(f\"No data found for {asset_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read merged data\n",
    "generation_data = clean_df('./Data/Merged/Generation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of assets: 51\n",
      "26 assets have the most data. They have 26304.0 data points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_32852\\2297823499.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  most_valuable_asset = asset_list.where(asset_list == asset_list[0]).dropna()\n",
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_32852\\2297823499.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"{len(most_valuable_asset)} assets have the most data. They have {most_valuable_asset[0]} data points.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Asset Name\n",
       "AKE1 McBride Lake Windfarm                 26304.0\n",
       "ARD1 Ardenville                            26304.0\n",
       "BSR1 Blackspring Ridge                     26304.0\n",
       "BTR1 Blue Trail Wind                       26304.0\n",
       "BUL1 Bull Creek                            26304.0\n",
       "BUL2 Bull Creek                            26304.0\n",
       "CR1 ARM2262 Castle River                   26304.0\n",
       "Cowley Ridge                               26304.0\n",
       "CRR1 Enel Alberta Castle Rock Wind Farm    26304.0\n",
       "CRR2 Castle Rock Ridge 2                   26304.0\n",
       "GWW1 GW Wind #1                            26304.0\n",
       "HAL1 Halkirk Wind Power Facility           26304.0\n",
       "IEW1 Summerview Phase 1                    26304.0\n",
       "IEW2 Summerview Phase 2                    26304.0\n",
       "KHW1 Kettles Hill Wind                     26304.0\n",
       "NEP1 Ghost Pine                            26304.0\n",
       "OWF1 Oldman 2 Wind Farm 1                  26304.0\n",
       "RIV1 Riverview                             26304.0\n",
       "RTL1 Rattlesnake Ridge Wind                26304.0\n",
       "SCR2 Magrath                               26304.0\n",
       "SCR3 Suncor Chin Chute                     26304.0\n",
       "SCR4 Wintering Hills                       26304.0\n",
       "TAB1 Enmax Taber                           26304.0\n",
       "WHT1 Whitla                                26304.0\n",
       "WHT2 Whitla 2                              26304.0\n",
       "WRW1 Windrise                              26304.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_assets_with_most_data(generation_data,'wind')\n",
    "# x = generation_data.where(generation_data[\"Planning Area\"] == 47).dropna()\n",
    "# x.where(x[\"Fuel Type\"] == \"WIND\").dropna()\n",
    "# generation_data.where(generation_data['Fuel Type'] == 'SOLAR').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume                0\n",
      "Maximum Capability    0\n",
      "System Capability     0\n",
      "Date                  0\n",
      "dtype: int64\n",
      "Processed CSV file saved successfully!\n",
      "Miissing values filled\n",
      "Data for AKE1 McBride Lake Windfarm has 26304 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_32852\\1683497974.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  asset_data['Date'] = pd.to_datetime(asset_data['Date (MST)'], errors='coerce')\n",
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_32852\\1683497974.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  asset_data.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_32852\\1683497974.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  asset_data.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\nabil\\AppData\\Local\\Temp\\ipykernel_32852\\1683497974.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  asset_data.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "asset_gen_data = get_asset_data(generation_data, asset_name, save_process_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The merged file has 26301 rows.\n",
      "Volume                          0\n",
      "Maximum Capability              0\n",
      "System Capability               0\n",
      "Date                            0\n",
      "Air Temp. Inst. (Â°C)           0\n",
      "Humidity Inst. (%)              0\n",
      "Incoming Solar Rad. (W/m2)      0\n",
      "Precip. (mm)                    0\n",
      "Wind Speed 10 m Syno. (km/h)    0\n",
      "Wind Dir. 10 m Syno. (Â°)       0\n",
      "Wind Speed 10 m Avg. (km/h)     0\n",
      "Wind Dir. 10 m Avg. (Â°)        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(asset_gen_data, weather_data, on='Date', how='inner')\n",
    "merged_rows = merged_df.shape[0]\n",
    "print(f\"The merged file has {merged_rows} rows.\")\n",
    "print(merged_df.isna().sum())\n",
    "merged_df.to_csv(f\"./Data/Merged/{asset_name}_Processed_and_Data.csv\", index=False)  # `index=False` prevents writing index column to CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enel678",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
